{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\farda\\AppData\\Local\\Temp\\ipykernel_5088\\2196626750.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{column}_auc'] = auc_values\n",
      "C:\\Users\\farda\\AppData\\Local\\Temp\\ipykernel_5088\\2196626750.py:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{column}_peaks'] = peaks_values\n",
      "C:\\Users\\farda\\AppData\\Local\\Temp\\ipykernel_5088\\2196626750.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{column}_mean'] = mean_values\n",
      "C:\\Users\\farda\\AppData\\Local\\Temp\\ipykernel_5088\\2196626750.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{column}_std'] = std_values\n",
      "C:\\Users\\farda\\AppData\\Local\\Temp\\ipykernel_5088\\2196626750.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{column}_min'] = min_values\n",
      "C:\\Users\\farda\\AppData\\Local\\Temp\\ipykernel_5088\\2196626750.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{column}_max'] = max_values\n",
      "C:\\Users\\farda\\AppData\\Local\\Temp\\ipykernel_5088\\2196626750.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{column}_auc'] = auc_values\n",
      "C:\\Users\\farda\\AppData\\Local\\Temp\\ipykernel_5088\\2196626750.py:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{column}_peaks'] = peaks_values\n",
      "C:\\Users\\farda\\AppData\\Local\\Temp\\ipykernel_5088\\2196626750.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features['class'] = data['class'][::60].values  # Keep every 60th value for class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Right Lower Leg x_mean  Right Lower Leg x_std  Right Lower Leg x_min  \\\n",
      "0                0.025233               0.194693              -0.337957   \n",
      "1               -0.269307               1.554992              -5.189986   \n",
      "2                0.377257               1.978953              -3.352960   \n",
      "3               -0.078073               0.746149              -1.402395   \n",
      "4               -0.080371               0.690406              -1.114383   \n",
      "\n",
      "   Right Lower Leg x_max  Right Lower Leg x_auc  Right Lower Leg x_peaks  \\\n",
      "0               0.685027               1.393713                       14   \n",
      "1               3.332500             -15.477592                       15   \n",
      "2              10.069568              24.355878                       14   \n",
      "3               1.907961              -4.117675                       15   \n",
      "4               2.388370              -4.116791                       14   \n",
      "\n",
      "   Right Lower Leg y_mean  Right Lower Leg y_std  Right Lower Leg y_min  \\\n",
      "0                0.008460               0.365897              -0.738636   \n",
      "1               -0.170189               3.040500              -8.166907   \n",
      "2                0.168026               1.804456              -3.320319   \n",
      "3                0.021141               1.018189              -2.335902   \n",
      "4               -0.025777               0.889627              -2.495246   \n",
      "\n",
      "   Right Lower Leg y_max  ...  RMS_XYZ_Left_max  RMS_XYZ_Left_auc  \\\n",
      "0               0.801465  ...          1.949838         12.517192   \n",
      "1              10.669277  ...          3.992627         84.035900   \n",
      "2               5.657845  ...          5.618483        100.959848   \n",
      "3               2.811169  ...          2.162978         44.863215   \n",
      "4               1.814754  ...          2.431476         62.686779   \n",
      "\n",
      "   RMS_XYZ_Left_peaks  Roll_Left_mean  Roll_Left_std  Roll_Left_min  \\\n",
      "0                  14       16.349236      49.221843     -83.112235   \n",
      "1                  12       -5.842137      49.187496     -83.191461   \n",
      "2                  14        2.062031      52.858357     -76.926243   \n",
      "3                  15        2.582602      47.109900     -83.792941   \n",
      "4                  15       -7.619793      53.144510     -84.877858   \n",
      "\n",
      "   Roll_Left_max  Roll_Left_auc  Roll_Left_peaks  class  \n",
      "0      85.055449     969.208903               15      0  \n",
      "1      84.213335    -385.468768               11      0  \n",
      "2      85.819609     127.857222               16      0  \n",
      "3      79.244135     174.774240               13      0  \n",
      "4      83.693236    -430.857461               13      0  \n",
      "\n",
      "[5 rows x 109 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Load the combined dataset\n",
    "file_path = r'C:\\week 3\\ampc2\\final_combined_data_lower_leg.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Columns to calculate features for (Columns 2-19)\n",
    "columns_to_analyze = data.columns[1:-1]  # Exclude Frame and Class columns\n",
    "\n",
    "# Initialize an empty DataFrame to store the statistical features\n",
    "features = pd.DataFrame()\n",
    "\n",
    "# Iterate over each column and calculate the statistical features\n",
    "for column in columns_to_analyze:\n",
    "    # Reshape data into chunks of 60 frames (1 minute)\n",
    "    reshaped_data = data[column].values.reshape(-1, 60)\n",
    "    \n",
    "    # Calculate statistical features for each chunk\n",
    "    mean_values = np.mean(reshaped_data, axis=1)\n",
    "    std_values = np.std(reshaped_data, axis=1)\n",
    "    min_values = np.min(reshaped_data, axis=1)\n",
    "    max_values = np.max(reshaped_data, axis=1)\n",
    "    \n",
    "    # Calculate Area Under the Curve (AUC) for each chunk using numpy's trapezoidal rule\n",
    "    auc_values = [np.trapz(chunk) for chunk in reshaped_data]\n",
    "    \n",
    "    # Calculate the number of peaks for each chunk\n",
    "    peaks_values = [len(find_peaks(chunk)[0]) for chunk in reshaped_data]\n",
    "    \n",
    "    # Append the calculated features to the features DataFrame\n",
    "    features[f'{column}_mean'] = mean_values\n",
    "    features[f'{column}_std'] = std_values\n",
    "    features[f'{column}_min'] = min_values\n",
    "    features[f'{column}_max'] = max_values\n",
    "    features[f'{column}_auc'] = auc_values\n",
    "    features[f'{column}_peaks'] = peaks_values\n",
    "\n",
    "# Add the class column back to the features DataFrame\n",
    "features['class'] = data['class'][::60].values  # Keep every 60th value for class\n",
    "\n",
    "# Preview the resulting features DataFrame\n",
    "print(features.head())\n",
    "\n",
    "# Save the features DataFrame to a new CSV file\n",
    "output_file_path = r'C:\\week 3\\ampc2\\feature_per_minute.csv'\n",
    "features.to_csv(output_file_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
